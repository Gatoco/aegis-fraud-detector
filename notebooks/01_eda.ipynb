{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0663f88",
   "metadata": {},
   "source": [
    "# IEEE-CIS Fraud Detection: Exploratory Data Analysis\n",
    "\n",
    "**Objective:** Comprehensive analysis of the IEEE-CIS fraud detection dataset to understand data characteristics, class imbalance, and feature distributions.\n",
    "\n",
    "**Dataset Overview:**\n",
    "- **Source:** IEEE Computational Intelligence Society\n",
    "- **Problem Type:** Binary classification (fraud detection)\n",
    "- **Target Variable:** `isFraud` (0: legitimate, 1: fraudulent)\n",
    "- **Expected Challenge:** Highly imbalanced dataset (~3.5% fraud rate)\n",
    "\n",
    "**Analysis Goals:**\n",
    "1. Data quality assessment and missing value patterns\n",
    "2. Target variable distribution and class imbalance quantification\n",
    "3. Feature type categorization and statistical summaries\n",
    "4. Temporal patterns and transaction characteristics\n",
    "5. Initial insights for feature engineering strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4174265d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Import essential libraries for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee5c741",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d181cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "DATA_PATH = Path('../data/01_raw')\n",
    "\n",
    "# Load training datasets\n",
    "print(\"Loading training transaction data...\")\n",
    "train_transaction = pd.read_csv(DATA_PATH / 'train_transaction.csv')\n",
    "\n",
    "print(\"Loading training identity data...\")\n",
    "train_identity = pd.read_csv(DATA_PATH / 'train_identity.csv')\n",
    "\n",
    "print(\"\\n=== Dataset Shapes ===\")\n",
    "print(f\"Transaction data: {train_transaction.shape}\")\n",
    "print(f\"Identity data: {train_identity.shape}\")\n",
    "\n",
    "# Memory usage assessment\n",
    "print(\"\\n=== Memory Usage ===\")\n",
    "print(f\"Transaction data: {train_transaction.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Identity data: {train_identity.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2451b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge transaction and identity data\n",
    "print(\"Merging transaction and identity datasets...\")\n",
    "train_data = train_transaction.merge(train_identity, on='TransactionID', how='left')\n",
    "\n",
    "print(f\"Merged dataset shape: {train_data.shape}\")\n",
    "print(f\"Memory usage after merge: {train_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Clean up memory\n",
    "del train_transaction, train_identity\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n=== Basic Dataset Information ===\")\n",
    "print(train_data.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f5620",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb92ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "print(\"=== Target Variable Distribution ===\")\n",
    "fraud_counts = train_data['isFraud'].value_counts()\n",
    "fraud_percentages = train_data['isFraud'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"Legitimate transactions (0): {fraud_counts[0]:,} ({fraud_percentages[0]:.2f}%)\")\n",
    "print(f\"Fraudulent transactions (1): {fraud_counts[1]:,} ({fraud_percentages[1]:.2f}%)\")\n",
    "print(f\"Class imbalance ratio: {fraud_counts[0] / fraud_counts[1]:.1f}:1\")\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Count plot\n",
    "fraud_counts.plot(kind='bar', ax=ax1, color=['#3498db', '#e74c3c'])\n",
    "ax1.set_title('Transaction Count by Class', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Class (0: Legitimate, 1: Fraudulent)')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Percentage plot\n",
    "fraud_percentages.plot(kind='bar', ax=ax2, color=['#3498db', '#e74c3c'])\n",
    "ax2.set_title('Transaction Percentage by Class', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Class (0: Legitimate, 1: Fraudulent)')\n",
    "ax2.set_ylabel('Percentage (%)')\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Add value labels on bars\n",
    "for ax in [ax1, ax2]:\n",
    "    for i, v in enumerate(ax.patches):\n",
    "        height = v.get_height()\n",
    "        if ax == ax1:\n",
    "            ax.text(v.get_x() + v.get_width()/2., height + height*0.01, f'{int(height):,}',\n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "        else:\n",
    "            ax.text(v.get_x() + v.get_width()/2., height + height*0.01, f'{height:.2f}%',\n",
    "                   ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966363ff",
   "metadata": {},
   "source": [
    "## 3. Feature Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33afb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize features by type\n",
    "print(\"=== Feature Type Analysis ===\")\n",
    "\n",
    "# Get data types\n",
    "dtypes_df = pd.DataFrame({\n",
    "    'Feature': train_data.columns,\n",
    "    'Type': train_data.dtypes.values,\n",
    "    'Non_Null_Count': train_data.count().values,\n",
    "    'Null_Count': train_data.isnull().sum().values,\n",
    "    'Null_Percentage': (train_data.isnull().sum() / len(train_data) * 100).values\n",
    "})\n",
    "\n",
    "# Categorize features\n",
    "numerical_features = dtypes_df[dtypes_df['Type'].isin(['int64', 'float64'])]['Feature'].tolist()\n",
    "categorical_features = dtypes_df[dtypes_df['Type'] == 'object']['Feature'].tolist()\n",
    "\n",
    "# Remove target and ID from numerical features\n",
    "if 'isFraud' in numerical_features:\n",
    "    numerical_features.remove('isFraud')\n",
    "if 'TransactionID' in numerical_features:\n",
    "    numerical_features.remove('TransactionID')\n",
    "\n",
    "print(f\"Total features: {len(train_data.columns)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Target variable: isFraud\")\n",
    "print(f\"Identifier: TransactionID\")\n",
    "\n",
    "# Display features with high missing values\n",
    "high_missing = dtypes_df[dtypes_df['Null_Percentage'] > 50].sort_values('Null_Percentage', ascending=False)\n",
    "print(f\"\\nFeatures with >50% missing values: {len(high_missing)}\")\n",
    "if len(high_missing) > 0:\n",
    "    print(high_missing[['Feature', 'Null_Percentage']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28470726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing value patterns\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Calculate missing percentages\n",
    "missing_percentages = (train_data.isnull().sum() / len(train_data) * 100).sort_values(ascending=False)\n",
    "features_with_missing = missing_percentages[missing_percentages > 0]\n",
    "\n",
    "if len(features_with_missing) > 0:\n",
    "    # Plot top 30 features with missing values\n",
    "    top_missing = features_with_missing.head(30)\n",
    "    \n",
    "    plt.barh(range(len(top_missing)), top_missing.values, color='#e74c3c')\n",
    "    plt.yticks(range(len(top_missing)), top_missing.index)\n",
    "    plt.xlabel('Missing Percentage (%)')\n",
    "    plt.title('Top 30 Features with Missing Values', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for i, v in enumerate(top_missing.values):\n",
    "        plt.text(v + 1, i, f'{v:.1f}%', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSummary: {len(features_with_missing)} features have missing values\")\n",
    "    print(f\"Range: {features_with_missing.min():.2f}% to {features_with_missing.max():.2f}%\")\n",
    "else:\n",
    "    print(\"No missing values found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b3106",
   "metadata": {},
   "source": [
    "## 4. Transaction Amount Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57192aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze TransactionAmt distribution\n",
    "print(\"=== Transaction Amount Analysis ===\")\n",
    "\n",
    "# Basic statistics\n",
    "amount_stats = train_data['TransactionAmt'].describe()\n",
    "print(\"Transaction Amount Statistics:\")\n",
    "print(amount_stats)\n",
    "\n",
    "# Fraud vs legitimate amounts\n",
    "fraud_amounts = train_data[train_data['isFraud'] == 1]['TransactionAmt']\n",
    "legit_amounts = train_data[train_data['isFraud'] == 0]['TransactionAmt']\n",
    "\n",
    "print(f\"\\nFraudulent transactions - Mean: ${fraud_amounts.mean():.2f}, Median: ${fraud_amounts.median():.2f}\")\n",
    "print(f\"Legitimate transactions - Mean: ${legit_amounts.mean():.2f}, Median: ${legit_amounts.median():.2f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Overall distribution (log scale)\n",
    "train_data['TransactionAmt'].apply(np.log1p).hist(bins=50, ax=ax1, alpha=0.7, color='#3498db')\n",
    "ax1.set_title('Transaction Amount Distribution (Log Scale)', fontweight='bold')\n",
    "ax1.set_xlabel('Log(Transaction Amount + 1)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# Box plot by fraud status\n",
    "train_data.boxplot(column='TransactionAmt', by='isFraud', ax=ax2)\n",
    "ax2.set_title('Transaction Amount by Fraud Status', fontweight='bold')\n",
    "ax2.set_xlabel('Fraud Status (0: Legitimate, 1: Fraudulent)')\n",
    "ax2.set_ylabel('Transaction Amount')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Fraud vs legitimate distributions\n",
    "fraud_amounts.apply(np.log1p).hist(bins=50, ax=ax3, alpha=0.7, color='#e74c3c', label='Fraudulent')\n",
    "legit_amounts.apply(np.log1p).hist(bins=50, ax=ax3, alpha=0.7, color='#3498db', label='Legitimate')\n",
    "ax3.set_title('Amount Distribution by Class (Log Scale)', fontweight='bold')\n",
    "ax3.set_xlabel('Log(Transaction Amount + 1)')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.legend()\n",
    "\n",
    "# Amount ranges analysis\n",
    "amount_ranges = pd.cut(train_data['TransactionAmt'], \n",
    "                      bins=[0, 50, 100, 500, 1000, 5000, float('inf')], \n",
    "                      labels=['$0-50', '$50-100', '$100-500', '$500-1K', '$1K-5K', '$5K+'])\n",
    "fraud_by_range = train_data.groupby(amount_ranges)['isFraud'].agg(['count', 'sum', 'mean'])\n",
    "fraud_by_range.columns = ['Total_Transactions', 'Fraud_Count', 'Fraud_Rate']\n",
    "fraud_by_range['Fraud_Rate'].plot(kind='bar', ax=ax4, color='#e74c3c')\n",
    "ax4.set_title('Fraud Rate by Transaction Amount Range', fontweight='bold')\n",
    "ax4.set_xlabel('Transaction Amount Range')\n",
    "ax4.set_ylabel('Fraud Rate')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFraud Rate by Amount Range:\")\n",
    "print(fraud_by_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab73bf6",
   "metadata": {},
   "source": [
    "## 5. Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical features\n",
    "print(\"=== Categorical Features Analysis ===\")\n",
    "\n",
    "if len(categorical_features) > 0:\n",
    "    print(f\"Found {len(categorical_features)} categorical features:\")\n",
    "    \n",
    "    cat_analysis = []\n",
    "    for feature in categorical_features:\n",
    "        unique_count = train_data[feature].nunique()\n",
    "        missing_pct = train_data[feature].isnull().sum() / len(train_data) * 100\n",
    "        most_common = train_data[feature].mode().iloc[0] if unique_count > 0 else 'N/A'\n",
    "        \n",
    "        cat_analysis.append({\n",
    "            'Feature': feature,\n",
    "            'Unique_Values': unique_count,\n",
    "            'Missing_Percentage': missing_pct,\n",
    "            'Most_Common': most_common\n",
    "        })\n",
    "    \n",
    "    cat_df = pd.DataFrame(cat_analysis)\n",
    "    print(cat_df)\n",
    "    \n",
    "    # Analyze top categorical features with reasonable cardinality\n",
    "    analyzable_cats = cat_df[(cat_df['Unique_Values'] > 1) & \n",
    "                            (cat_df['Unique_Values'] <= 20) & \n",
    "                            (cat_df['Missing_Percentage'] < 80)]\n",
    "    \n",
    "    if len(analyzable_cats) > 0:\n",
    "        print(f\"\\nAnalyzing {len(analyzable_cats)} categorical features with reasonable cardinality...\")\n",
    "        \n",
    "        # Plot fraud rates for top categorical features\n",
    "        n_plots = min(4, len(analyzable_cats))\n",
    "        if n_plots > 0:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            axes = axes.ravel()\n",
    "            \n",
    "            for i, feature in enumerate(analyzable_cats['Feature'].head(n_plots)):\n",
    "                fraud_by_cat = train_data.groupby(feature)['isFraud'].agg(['count', 'mean']).reset_index()\n",
    "                fraud_by_cat = fraud_by_cat[fraud_by_cat['count'] >= 100]  # Filter low-frequency categories\n",
    "                \n",
    "                if len(fraud_by_cat) > 0:\n",
    "                    fraud_by_cat.plot(x=feature, y='mean', kind='bar', ax=axes[i], color='#e74c3c')\n",
    "                    axes[i].set_title(f'Fraud Rate by {feature}', fontweight='bold')\n",
    "                    axes[i].set_ylabel('Fraud Rate')\n",
    "                    axes[i].tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Hide empty subplots\n",
    "            for i in range(n_plots, 4):\n",
    "                axes[i].set_visible(False)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"No categorical features found in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864548c9",
   "metadata": {},
   "source": [
    "## 6. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis with target variable\n",
    "print(\"=== Feature Correlation with Target ===\")\n",
    "\n",
    "# Calculate correlations with target (only numerical features)\n",
    "numeric_cols = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'TransactionID' in numeric_cols:\n",
    "    numeric_cols.remove('TransactionID')\n",
    "\n",
    "correlations = train_data[numeric_cols].corr()['isFraud'].abs().sort_values(ascending=False)\n",
    "correlations = correlations.drop('isFraud')  # Remove self-correlation\n",
    "\n",
    "print(\"Top 20 features most correlated with fraud:\")\n",
    "top_corr = correlations.head(20)\n",
    "print(top_corr)\n",
    "\n",
    "# Visualize top correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_corr.plot(kind='barh', color='#3498db')\n",
    "plt.title('Top 20 Features Correlated with Fraud', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Absolute Correlation with isFraud')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add correlation values as text\n",
    "for i, v in enumerate(top_corr.values):\n",
    "    plt.text(v + 0.001, i, f'{v:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature correlation heatmap (top correlated features)\n",
    "if len(top_corr) >= 10:\n",
    "    top_features = ['isFraud'] + top_corr.head(10).index.tolist()\n",
    "    corr_matrix = train_data[top_features].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Correlation Matrix: Top Features + Target', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1caac",
   "metadata": {},
   "source": [
    "## 7. Data Quality Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality summary\n",
    "print(\"=== COMPREHENSIVE DATA QUALITY SUMMARY ===\")\n",
    "print(f\"Dataset Shape: {train_data.shape}\")\n",
    "print(f\"Memory Usage: {train_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print()\n",
    "\n",
    "print(\"CLASS IMBALANCE:\")\n",
    "fraud_rate = train_data['isFraud'].mean() * 100\n",
    "print(f\"- Fraud Rate: {fraud_rate:.2f}%\")\n",
    "print(f\"- Imbalance Ratio: {(100-fraud_rate)/fraud_rate:.1f}:1\")\n",
    "print(f\"- Classification Challenge: HIGH (severely imbalanced)\")\n",
    "print()\n",
    "\n",
    "print(\"FEATURE COMPOSITION:\")\n",
    "print(f\"- Total Features: {len(train_data.columns) - 1}\")\n",
    "print(f\"- Numerical Features: {len(numerical_features)}\")\n",
    "print(f\"- Categorical Features: {len(categorical_features)}\")\n",
    "print()\n",
    "\n",
    "print(\"MISSING VALUES:\")\n",
    "features_with_missing = (train_data.isnull().sum() > 0).sum()\n",
    "avg_missing = train_data.isnull().sum().mean()\n",
    "max_missing = train_data.isnull().sum().max() / len(train_data) * 100\n",
    "print(f\"- Features with Missing Values: {features_with_missing}\")\n",
    "print(f\"- Maximum Missing Percentage: {max_missing:.1f}%\")\n",
    "print(f\"- Data Quality Challenge: {'HIGH' if max_missing > 50 else 'MODERATE' if max_missing > 20 else 'LOW'}\")\n",
    "print()\n",
    "\n",
    "print(\"TOP PREDICTIVE FEATURES:\")\n",
    "if len(correlations) > 0:\n",
    "    for i, (feature, corr) in enumerate(correlations.head(5).items()):\n",
    "        print(f\"- {i+1}. {feature}: {corr:.3f}\")\n",
    "else:\n",
    "    print(\"- No significant correlations found\")\n",
    "print()\n",
    "\n",
    "print(\"=== RECOMMENDED NEXT STEPS ===\")\n",
    "print(\"1. FEATURE ENGINEERING:\")\n",
    "print(\"   - Create temporal features from TransactionDT\")\n",
    "print(\"   - Engineer interaction features between high-correlation variables\")\n",
    "print(\"   - Develop aggregation features (velocity, frequency patterns)\")\n",
    "print(\"   - Handle missing values strategically (imputation vs. indicator variables)\")\n",
    "print()\n",
    "\n",
    "print(\"2. PREPROCESSING PIPELINE:\")\n",
    "print(\"   - Implement robust scaling for numerical features\")\n",
    "print(\"   - Encode categorical variables (target encoding for high cardinality)\")\n",
    "print(\"   - Feature selection based on importance and correlation\")\n",
    "print()\n",
    "\n",
    "print(\"3. IMBALANCED LEARNING STRATEGY:\")\n",
    "print(\"   - Implement stratified sampling for train/validation split\")\n",
    "print(\"   - Apply SMOTE or ADASYN for synthetic minority oversampling\")\n",
    "print(\"   - Use cost-sensitive learning with class weights\")\n",
    "print(\"   - Focus on Precision-Recall metrics over accuracy\")\n",
    "print()\n",
    "\n",
    "print(\"4. MODEL DEVELOPMENT:\")\n",
    "print(\"   - Start with Logistic Regression baseline\")\n",
    "print(\"   - Progress to ensemble methods (XGBoost, LightGBM)\")\n",
    "print(\"   - Implement proper cross-validation strategy\")\n",
    "print(\"   - Target AUC-ROC ≥ 0.87 as specified\")\n",
    "\n",
    "# Save feature lists for next notebooks\n",
    "feature_info = {\n",
    "    'numerical_features': numerical_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'top_correlated_features': correlations.head(20).index.tolist() if len(correlations) > 0 else [],\n",
    "    'high_missing_features': dtypes_df[dtypes_df['Null_Percentage'] > 50]['Feature'].tolist()\n",
    "}\n",
    "\n",
    "print(f\"\\n=== ANALYSIS COMPLETE ===\")\n",
    "print(f\"Feature information saved for subsequent analysis notebooks\")\n",
    "print(f\"Ready to proceed with feature engineering and preprocessing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
