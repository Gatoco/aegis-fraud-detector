{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cd59cb",
   "metadata": {},
   "source": [
    "# Aegis Fraud Detector - Exploratory Data Analysis (EDA)\n",
    "\n",
    "**Sprint 1.1**: Análisis Exploratorio de Datos Riguroso  \n",
    "**Dataset**: IEEE-CIS Fraud Detection  \n",
    "**Fecha**: Agosto 2025  \n",
    "**Metodología**: EDA basado en scripts versionados para garantizar reproducibilidad\n",
    "\n",
    "## Objetivos del Análisis\n",
    "\n",
    "1. **Comprensión profunda del dataset**: Estructura, tipos de datos, y calidad general\n",
    "2. **Análisis de la variable objetivo**: Prevalencia de fraude y patrones temporales\n",
    "3. **Exploración de features**: Distribuciones, correlaciones, y valores faltantes\n",
    "4. **Identificación de insights**: Patrones, anomalías, y oportunidades de feature engineering\n",
    "5. **Documentación de hallazgos**: Base para decisiones de modelado futuras\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45421445",
   "metadata": {},
   "source": [
    "## 1. Configuración e Importación de Librerías\n",
    "\n",
    "**Nota importante**: Este notebook utiliza exclusivamente funciones del módulo `src/data/exploration.py` para garantizar la reproducibilidad y el versionado del código de análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16271284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuración completada\n",
      "📂 Directorio de trabajo: c:\\Users\\Gat\\Documents\\GitHub\\aegis-fraud-detector\\notebooks\n",
      "📊 Versión de pandas: 2.3.1\n",
      "🔍 Versión de numpy: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "# Configuración del entorno\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Agregar el directorio src al path para importar nuestros módulos\n",
    "project_root = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "# Importaciones estándar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Importar nuestro módulo de exploración personalizado\n",
    "from data.exploration import FraudDataExplorer, create_eda_plotting_functions\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar pandas para mostrar más columnas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✅ Configuración completada\")\n",
    "print(f\"📂 Directorio de trabajo: {os.getcwd()}\")\n",
    "print(f\"📊 Versión de pandas: {pd.__version__}\")\n",
    "print(f\"🔍 Versión de numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b4858",
   "metadata": {},
   "source": [
    "## 2. Inicialización del Explorador de Datos\n",
    "\n",
    "Utilizamos la clase `FraudDataExplorer` para realizar todos los análisis de manera estructurada y reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6d42b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Explorador inicializado\n",
      "📁 Ruta de datos: c:\\Users\\Gat\\Documents\\GitHub\\aegis-fraud-detector\\data\\01_raw\n",
      "📋 Verificando archivos disponibles...\n",
      "  ✅ train_transaction.csv (651.7 MB)\n",
      "  ✅ train_identity.csv (25.3 MB)\n",
      "  ✅ test_transaction.csv (584.8 MB)\n",
      "  ✅ test_identity.csv (24.6 MB)\n"
     ]
    }
   ],
   "source": [
    "# Inicializar el explorador de datos\n",
    "data_path = project_root / \"data\" / \"01_raw\"\n",
    "explorer = FraudDataExplorer(data_path=str(data_path))\n",
    "\n",
    "print(f\"🔍 Explorador inicializado\")\n",
    "print(f\"📁 Ruta de datos: {data_path}\")\n",
    "print(f\"📋 Verificando archivos disponibles...\")\n",
    "\n",
    "# Verificar que los archivos existan\n",
    "required_files = [\n",
    "    \"train_transaction.csv\",\n",
    "    \"train_identity.csv\",\n",
    "    \"test_transaction.csv\",\n",
    "    \"test_identity.csv\"\n",
    "]\n",
    "\n",
    "for file in required_files:\n",
    "    file_path = data_path / file\n",
    "    if file_path.exists():\n",
    "        file_size = file_path.stat().st_size / (1024**2)  # MB\n",
    "        print(f\"  ✅ {file} ({file_size:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ❌ {file} - No encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a8a26",
   "metadata": {},
   "source": [
    "## 3. Carga y Revisión General de Datasets\n",
    "\n",
    "Cargamos todos los datasets del IEEE-CIS Fraud Detection y realizamos una primera inspección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fa8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar todos los datasets\n",
    "print(\"🔄 Cargando datasets...\")\n",
    "datasets = explorer.load_datasets()\n",
    "\n",
    "print(\"\\n📊 RESUMEN DE DATASETS CARGADOS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    memory_usage = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  📏 Dimensiones: {df.shape}\")\n",
    "    print(f\"  💾 Memoria: {memory_usage:.1f} MB\")\n",
    "    print(f\"  🔢 Tipos de datos: {dict(df.dtypes.value_counts())}\")\n",
    "    \n",
    "    if name == 'merged_train':\n",
    "        print(f\"  🎯 Variable objetivo (isFraud): {df['isFraud'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"\\n✅ Carga completada exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ab29f",
   "metadata": {},
   "source": [
    "## 4. Análisis Comprehensive del Dataset\n",
    "\n",
    "Realizamos un análisis detallado de la estructura y composición del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8840df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar análisis de overview completo\n",
    "print(\"🔍 Realizando análisis comprehensive del dataset...\")\n",
    "overview_results = explorer.analyze_dataset_overview()\n",
    "\n",
    "print(\"\\n📋 ANÁLISIS DE ESTRUCTURA DEL DATASET:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mostrar dimensiones\n",
    "print(\"\\n📏 DIMENSIONES:\")\n",
    "for dataset_name, shape in overview_results['shape'].items():\n",
    "    print(f\"  {dataset_name}: {shape[0]:,} filas × {shape[1]} columnas\")\n",
    "\n",
    "# Mostrar uso de memoria\n",
    "print(\"\\n💾 USO DE MEMORIA:\")\n",
    "for dataset_name, memory in overview_results['memory_usage'].items():\n",
    "    print(f\"  {dataset_name}: {memory:.1f} MB\")\n",
    "\n",
    "# Mostrar categorías de features\n",
    "print(\"\\n🏷️ CATEGORÍAS DE FEATURES:\")\n",
    "for category, count in overview_results['feature_counts'].items():\n",
    "    print(f\"  {category}: {count} features\")\n",
    "\n",
    "# Detalles de las categorías más importantes\n",
    "print(\"\\n🔍 FEATURES POR CATEGORÍA:\")\n",
    "important_categories = ['card_features', 'categorical_features', 'continuous_features']\n",
    "for category in important_categories:\n",
    "    features = overview_results['feature_categories'].get(category, [])\n",
    "    print(f\"\\n  {category.upper()}:\")\n",
    "    print(f\"    Cantidad: {len(features)}\")\n",
    "    if features:\n",
    "        print(f\"    Ejemplos: {features[:5]}\")\n",
    "        if len(features) > 5:\n",
    "            print(f\"    ... y {len(features) - 5} más\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07ca64",
   "metadata": {},
   "source": [
    "## 5. Análisis de Valores Faltantes\n",
    "\n",
    "Investigación profunda de los patrones de valores faltantes en el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis comprehensive de valores faltantes\n",
    "print(\"🔍 Analizando patrones de valores faltantes...\")\n",
    "missing_results = explorer.analyze_missing_values()\n",
    "\n",
    "print(\"\\n🕳️ ANÁLISIS DE VALORES FALTANTES:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for dataset_name, missing_info in missing_results.items():\n",
    "    print(f\"\\n📊 {dataset_name.upper()}:\")\n",
    "    print(f\"  Total features: {missing_info['total_features']}\")\n",
    "    print(f\"  Features completas: {missing_info['complete_features']}\")\n",
    "    print(f\"  Features con valores faltantes: {missing_info['features_with_missing']}\")\n",
    "    print(f\"  Features con >90% faltantes: {missing_info['heavily_missing_features']}\")\n",
    "    print(f\"  Features con 50-90% faltantes: {missing_info['moderately_missing_features']}\")\n",
    "    print(f\"  Features con <50% faltantes: {missing_info['lightly_missing_features']}\")\n",
    "    \n",
    "    # Mostrar top 10 features con más valores faltantes\n",
    "    top_missing = missing_info['missing_summary'].head(10)\n",
    "    if len(top_missing) > 0:\n",
    "        print(f\"\\n  🔝 TOP 10 FEATURES CON MÁS VALORES FALTANTES:\")\n",
    "        for _, row in top_missing.iterrows():\n",
    "            print(f\"    {row['column']}: {row['missing_percentage']:.1f}% ({row['missing_count']:,} valores)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679e6e1b",
   "metadata": {},
   "source": [
    "## 6. Análisis Profundo de la Variable Objetivo (isFraud)\n",
    "\n",
    "**Investigación exhaustiva del fraude**: Prevalencia, patrones temporales, y características distintivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c287cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis profundo de la variable objetivo\n",
    "print(\"🎯 Analizando variable objetivo (isFraud) en profundidad...\")\n",
    "target_results = explorer.analyze_target_variable()\n",
    "\n",
    "print(\"\\n🎯 ANÁLISIS DE LA VARIABLE OBJETIVO:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Distribución básica\n",
    "basic_dist = target_results['basic_distribution']\n",
    "print(f\"\\n📊 DISTRIBUCIÓN BÁSICA:\")\n",
    "print(f\"  Total transacciones: {basic_dist['total_transactions']:,}\")\n",
    "print(f\"  Transacciones fraudulentas: {basic_dist['fraud_transactions']:,}\")\n",
    "print(f\"  Transacciones legítimas: {basic_dist['legitimate_transactions']:,}\")\n",
    "print(f\"  Tasa de fraude: {basic_dist['fraud_rate']:.3f}%\")\n",
    "print(f\"  Ratio de desbalance: 1:{basic_dist['class_imbalance_ratio']:.0f}\")\n",
    "\n",
    "# Análisis de cantidad de transacciones\n",
    "if 'amount_patterns' in target_results:\n",
    "    amount_patterns = target_results['amount_patterns']\n",
    "    print(f\"\\n💰 PATRONES EN MONTOS DE TRANSACCIÓN:\")\n",
    "    print(f\"  Monto mediano - Fraude: ${amount_patterns['median_fraud_amount']:.2f}\")\n",
    "    print(f\"  Monto mediano - Legítimo: ${amount_patterns['median_legit_amount']:.2f}\")\n",
    "    print(f\"  Correlación monto-fraude: {amount_patterns['amount_correlation_with_fraud']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  📈 ESTADÍSTICAS DE MONTOS FRAUDULENTOS:\")\n",
    "    fraud_stats = amount_patterns['fraud_amount_stats']\n",
    "    print(f\"    Media: ${fraud_stats['mean']:.2f}\")\n",
    "    print(f\"    Mediana: ${fraud_stats['50%']:.2f}\")\n",
    "    print(f\"    Desv. estándar: ${fraud_stats['std']:.2f}\")\n",
    "    print(f\"    Rango: ${fraud_stats['min']:.2f} - ${fraud_stats['max']:.2f}\")\n",
    "\n",
    "# Análisis temporal\n",
    "if 'temporal_patterns' in target_results:\n",
    "    temporal = target_results['temporal_patterns']\n",
    "    print(f\"\\n⏰ PATRONES TEMPORALES:\")\n",
    "    print(f\"  Hora pico de fraude: {temporal['peak_fraud_hour']:.0f}:00\")\n",
    "    print(f\"  Día pico de fraude: {int(temporal['peak_fraud_day'])} (0=Lunes, 6=Domingo)\")\n",
    "    print(f\"  Varianza de fraude por hora: {temporal['fraud_rate_variance_hourly']:.6f}\")\n",
    "    print(f\"  Varianza de fraude por día: {temporal['fraud_rate_variance_daily']:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f23c70",
   "metadata": {},
   "source": [
    "## 7. Visualización de Patrones de la Variable Objetivo\n",
    "\n",
    "Generamos visualizaciones para entender mejor los patrones de fraude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b570affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear funciones de plotting\n",
    "plotting_functions = create_eda_plotting_functions()\n",
    "\n",
    "# Visualizar distribución de la variable objetivo\n",
    "print(\"📈 Generando visualizaciones de la variable objetivo...\")\n",
    "\n",
    "# Gráfico de distribución del fraude\n",
    "fraud_counts = explorer.merged_train['isFraud'].value_counts()\n",
    "plotting_functions['plot_target_distribution'](fraud_counts)\n",
    "\n",
    "# Patrones temporales si están disponibles\n",
    "if 'temporal_patterns' in target_results:\n",
    "    temporal = target_results['temporal_patterns']\n",
    "    hourly_data = temporal['hourly_patterns']\n",
    "    daily_data = temporal['daily_patterns']\n",
    "    \n",
    "    plotting_functions['plot_temporal_patterns'](hourly_data, daily_data)\n",
    "    \n",
    "    # Mostrar datos temporales en formato tabular\n",
    "    print(\"\\n⏰ DATOS TEMPORALES DETALLADOS:\")\n",
    "    print(\"\\n📊 Fraude por hora del día:\")\n",
    "    print(hourly_data.round(4))\n",
    "    \n",
    "    print(\"\\n📊 Fraude por día de la semana:\")\n",
    "    daily_data['day_name'] = ['Lun', 'Mar', 'Mié', 'Jue', 'Vie', 'Sáb', 'Dom']\n",
    "    print(daily_data[['day_name', 'total_transactions', 'fraud_count', 'fraud_rate']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b44cb0a",
   "metadata": {},
   "source": [
    "## 8. Análisis de Distribuciones de Features\n",
    "\n",
    "Exploración de las distribuciones de las features más importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de distribuciones de features\n",
    "print(\"🔍 Analizando distribuciones de features clave...\")\n",
    "distribution_results = explorer.analyze_feature_distributions(sample_size=10000)\n",
    "\n",
    "print(\"\\n📊 ANÁLISIS DE DISTRIBUCIONES:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mostrar estadísticas de las features numéricas más importantes\n",
    "print(\"\\n🔢 TOP FEATURES NUMÉRICAS:\")\n",
    "numeric_features = {k: v for k, v in distribution_results.items() \n",
    "                   if k != 'categorical_analysis' and isinstance(v, dict)}\n",
    "\n",
    "# Ordenar por importancia (menos valores faltantes y más varianza)\n",
    "feature_importance = []\n",
    "for feature, stats in numeric_features.items():\n",
    "    if 'missing_percentage' in stats and 'std' in stats:\n",
    "        importance_score = (100 - stats['missing_percentage']) * np.log1p(stats['std'])\n",
    "        feature_importance.append((feature, importance_score, stats))\n",
    "\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nMostrando las top 10 features numéricas más relevantes:\")\n",
    "for i, (feature, score, stats) in enumerate(feature_importance[:10]):\n",
    "    print(f\"\\n{i+1}. {feature}:\")\n",
    "    print(f\"    Media: {stats.get('mean', 'N/A'):.4f}\")\n",
    "    print(f\"    Mediana: {stats.get('median', 'N/A'):.4f}\")\n",
    "    print(f\"    Desv. estándar: {stats.get('std', 'N/A'):.4f}\")\n",
    "    print(f\"    Skewness: {stats.get('skewness', 'N/A'):.4f}\")\n",
    "    print(f\"    Valores únicos: {stats.get('unique_values', 'N/A')}\")\n",
    "    print(f\"    Valores faltantes: {stats.get('missing_percentage', 'N/A'):.1f}%\")\n",
    "    print(f\"    Outliers: {stats.get('outlier_percentage', 'N/A'):.1f}%\")\n",
    "\n",
    "# Análisis de features categóricas\n",
    "if 'categorical_analysis' in distribution_results:\n",
    "    categorical_stats = distribution_results['categorical_analysis']\n",
    "    print(f\"\\n🏷️ FEATURES CATEGÓRICAS ANALIZADAS: {len(categorical_stats)}\")\n",
    "    \n",
    "    for feature, stats in list(categorical_stats.items())[:5]:\n",
    "        print(f\"\\n  {feature}:\")\n",
    "        print(f\"    Valores únicos: {stats['unique_values']}\")\n",
    "        print(f\"    Valor más frecuente: {stats['most_frequent']}\")\n",
    "        print(f\"    Valores faltantes: {stats['missing_percentage']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06bba37",
   "metadata": {},
   "source": [
    "## 9. Análisis de Correlaciones\n",
    "\n",
    "Exploración de las correlaciones entre features y con la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee2640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de correlaciones\n",
    "print(\"🔗 Analizando correlaciones entre features...\")\n",
    "correlation_results = explorer.analyze_correlations(method='pearson')\n",
    "\n",
    "print(\"\\n🔗 ANÁLISIS DE CORRELACIONES:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Correlaciones más fuertes con la variable objetivo\n",
    "if 'target_correlations' in correlation_results:\n",
    "    target_corrs = correlation_results['target_correlations']\n",
    "    \n",
    "    print(\"\\n🎯 CORRELACIONES MÁS FUERTES CON isFraud:\")\n",
    "    print(\"\\n  📈 Correlaciones positivas más altas:\")\n",
    "    for feature, corr in list(target_corrs['top_positive'].items())[:10]:\n",
    "        print(f\"    {feature}: {corr:.4f}\")\n",
    "    \n",
    "    print(\"\\n  📉 Correlaciones negativas más fuertes:\")\n",
    "    negative_corrs = target_corrs['top_negative']\n",
    "    for feature, corr in list(negative_corrs.items())[:10]:\n",
    "        if corr < 0:  # Solo mostrar correlaciones negativas\n",
    "            print(f\"    {feature}: {corr:.4f}\")\n",
    "\n",
    "# Multicolinealidad\n",
    "high_corrs = correlation_results.get('high_correlations', [])\n",
    "print(f\"\\n⚠️ MULTICOLINEALIDAD DETECTADA:\")\n",
    "print(f\"  Pares de features con correlación > 0.8: {len(high_corrs)}\")\n",
    "\n",
    "if high_corrs:\n",
    "    print(\"\\n  🔍 Ejemplos de alta correlación:\")\n",
    "    for pair in high_corrs[:10]:  # Mostrar primeros 10\n",
    "        print(f\"    {pair['feature1']} ↔ {pair['feature2']}: {pair['correlation']:.4f}\")\n",
    "    \n",
    "    if len(high_corrs) > 10:\n",
    "        print(f\"    ... y {len(high_corrs) - 10} pares más\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea976bd",
   "metadata": {},
   "source": [
    "## 10. Visualización de Correlaciones y Distribuciones\n",
    "\n",
    "Heatmaps y distribuciones clave para insights visuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0592652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar correlaciones\n",
    "print(\"📈 Generando visualizaciones de correlaciones...\")\n",
    "\n",
    "# Heatmap de correlaciones\n",
    "correlation_matrix = correlation_results['correlation_matrix']\n",
    "plotting_functions['plot_correlation_heatmap'](correlation_matrix)\n",
    "\n",
    "# Distribución de montos de transacción\n",
    "if 'TransactionAmt' in explorer.merged_train.columns:\n",
    "    print(\"\\n💰 Distribución de montos de transacción:\")\n",
    "    fraud_amounts = explorer.merged_train[explorer.merged_train['isFraud'] == 1]['TransactionAmt']\n",
    "    legit_amounts = explorer.merged_train[explorer.merged_train['isFraud'] == 0]['TransactionAmt']\n",
    "    \n",
    "    plotting_functions['plot_amount_distribution'](fraud_amounts, legit_amounts)\n",
    "    \n",
    "    # Estadísticas de montos\n",
    "    print(f\"\\n📊 Estadísticas de montos de transacción:\")\n",
    "    print(f\"\\nFRAUDULENTAS:\")\n",
    "    print(fraud_amounts.describe())\n",
    "    print(f\"\\nLEGÍTIMAS:\")\n",
    "    print(legit_amounts.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd1827",
   "metadata": {},
   "source": [
    "## 11. Visualización de Patrones de Valores Faltantes\n",
    "\n",
    "Matriz de valores faltantes para identificar patrones estructurales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7372c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar patrones de valores faltantes\n",
    "print(\"🕳️ Generando visualización de valores faltantes...\")\n",
    "\n",
    "# Matriz de valores faltantes\n",
    "missing_data = missing_results['merged_train']['missing_value_heatmap_data']\n",
    "\n",
    "# Tomar una muestra para visualización (demasiados datos para mostrar todos)\n",
    "sample_size = min(5000, len(missing_data))\n",
    "sample_missing = missing_data.sample(n=sample_size, random_state=42)\n",
    "\n",
    "plotting_functions['plot_missing_value_matrix'](sample_missing)\n",
    "\n",
    "# Análisis de patrones de missingness\n",
    "print(\"\\n🔍 ANÁLISIS DE PATRONES DE VALORES FALTANTES:\")\n",
    "missing_summary = missing_results['merged_train']['missing_summary']\n",
    "\n",
    "# Features completamente vacías\n",
    "completely_missing = missing_summary[missing_summary['missing_percentage'] > 99]\n",
    "print(f\"\\n❌ Features casi completamente vacías (>99% missing): {len(completely_missing)}\")\n",
    "if len(completely_missing) > 0:\n",
    "    print(\"  Ejemplos:\", completely_missing['column'].head().tolist())\n",
    "\n",
    "# Features con patrones específicos de missing\n",
    "high_missing = missing_summary[(missing_summary['missing_percentage'] > 80) & \n",
    "                              (missing_summary['missing_percentage'] <= 99)]\n",
    "print(f\"\\n⚠️ Features con alta proporción de valores faltantes (80-99%): {len(high_missing)}\")\n",
    "\n",
    "moderate_missing = missing_summary[(missing_summary['missing_percentage'] > 20) & \n",
    "                                  (missing_summary['missing_percentage'] <= 80)]\n",
    "print(f\"\\n📊 Features con proporción moderada de valores faltantes (20-80%): {len(moderate_missing)}\")\n",
    "\n",
    "low_missing = missing_summary[(missing_summary['missing_percentage'] > 0) & \n",
    "                             (missing_summary['missing_percentage'] <= 20)]\n",
    "print(f\"\\n✅ Features con baja proporción de valores faltantes (0-20%): {len(low_missing)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cbbe6",
   "metadata": {},
   "source": [
    "## 12. Generación de Reporte Comprehensive de EDA\n",
    "\n",
    "Generamos un reporte completo con todos los hallazgos y recomendaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02515847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar visualizaciones y reporte final\n",
    "print(\"📋 Generando reporte comprehensive de EDA...\")\n",
    "\n",
    "# Generar configuraciones de visualización\n",
    "visualizations = explorer.generate_eda_visualizations()\n",
    "\n",
    "# Generar reporte completo\n",
    "eda_report = explorer.generate_eda_report()\n",
    "\n",
    "print(\"\\n📋 REPORTE EJECUTIVO DE EDA:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Metadata del análisis\n",
    "metadata = eda_report['metadata']\n",
    "print(f\"\\n📅 Fecha de análisis: {metadata['analysis_date'][:19]}\")\n",
    "print(f\"📊 Dataset: {metadata['dataset_version']}\")\n",
    "print(f\"🔍 Versión de análisis: {metadata['analysis_version']}\")\n",
    "print(f\"📈 Features analizadas: {metadata['total_features_analyzed']}\")\n",
    "\n",
    "# Resumen ejecutivo\n",
    "exec_summary = eda_report['executive_summary']\n",
    "print(f\"\\n📋 RESUMEN EJECUTIVO:\")\n",
    "for key, value in exec_summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Recomendaciones clave\n",
    "recommendations = eda_report['recommendations']\n",
    "print(f\"\\n🎯 RECOMENDACIONES CLAVE:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "print(\"\\n✅ Análisis EDA completado exitosamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2e9066",
   "metadata": {},
   "source": [
    "## 13. Hallazgos Clave y Insights para Feature Engineering\n",
    "\n",
    "Documentación de los insights más importantes para las siguientes fases del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30068a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumir hallazgos clave para documentación\n",
    "print(\"🔍 HALLAZGOS CLAVE PARA FEATURE ENGINEERING:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Análisis de la clase objetivo\n",
    "fraud_rate = target_results['basic_distribution']['fraud_rate']\n",
    "class_ratio = target_results['basic_distribution']['class_imbalance_ratio']\n",
    "\n",
    "print(f\"\\n🎯 DESBALANCE DE CLASES:\")\n",
    "print(f\"  • Tasa de fraude extremadamente baja: {fraud_rate:.3f}%\")\n",
    "print(f\"  • Ratio de desbalance: 1:{class_ratio:.0f}\")\n",
    "print(f\"  • IMPLICACIÓN: Necesidad crítica de técnicas de balanceamento\")\n",
    "\n",
    "# Features con alta correlación\n",
    "if 'target_correlations' in correlation_results:\n",
    "    top_features = list(correlation_results['target_correlations']['strongest_correlations'].keys())[:5]\n",
    "    print(f\"\\n🔗 FEATURES MÁS PREDICTIVAS:\")\n",
    "    for i, feature in enumerate(top_features, 1):\n",
    "        corr_value = correlation_results['target_correlations']['strongest_correlations'][feature]\n",
    "        print(f\"  {i}. {feature}: {corr_value:.4f}\")\n",
    "\n",
    "# Patrones temporales\n",
    "if 'temporal_patterns' in target_results:\n",
    "    temporal = target_results['temporal_patterns']\n",
    "    print(f\"\\n⏰ PATRONES TEMPORALES:\")\n",
    "    print(f\"  • Hora pico de fraude: {temporal['peak_fraud_hour']:.0f}:00\")\n",
    "    print(f\"  • Variación horaria significativa: {temporal['fraud_rate_variance_hourly'] > 0.0001}\")\n",
    "    print(f\"  • IMPLICACIÓN: Features temporales son prometedoras\")\n",
    "\n",
    "# Análisis de valores faltantes\n",
    "heavily_missing = missing_results['merged_train']['heavily_missing_features']\n",
    "total_features = missing_results['merged_train']['total_features']\n",
    "\n",
    "print(f\"\\n🕳️ VALORES FALTANTES:\")\n",
    "print(f\"  • Features con >90% valores faltantes: {heavily_missing}/{total_features}\")\n",
    "print(f\"  • Proporción de features problemáticas: {(heavily_missing/total_features)*100:.1f}%\")\n",
    "print(f\"  • IMPLICACIÓN: Estrategia de limpieza y feature selection crítica\")\n",
    "\n",
    "# Multicolinealidad\n",
    "multicollinear_pairs = len(correlation_results.get('high_correlations', []))\n",
    "print(f\"\\n⚠️ MULTICOLINEALIDAD:\")\n",
    "print(f\"  • Pares de features altamente correlacionados: {multicollinear_pairs}\")\n",
    "print(f\"  • IMPLICACIÓN: Reducción de dimensionalidad necesaria\")\n",
    "\n",
    "# Guardado de datos para siguientes sprints\n",
    "print(f\"\\n💾 PREPARACIÓN PARA PRÓXIMOS SPRINTS:\")\n",
    "print(f\"  • Reporte EDA guardado en findings del explorador\")\n",
    "print(f\"  • Features clave identificadas para feature engineering\")\n",
    "print(f\"  • Estrategias de preprocesamiento definidas\")\n",
    "print(f\"  • Baseline para métricas de evaluación establecido\")\n",
    "\n",
    "print(\"\\n🎯 EDA RIGUROSO COMPLETADO - LISTO PARA SPRINT 1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2062315e",
   "metadata": {},
   "source": [
    "## 14. Exportación de Resultados\n",
    "\n",
    "Guardamos los resultados del análisis para uso en futuros notebooks y scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar resultados para uso futuro\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Crear directorio de resultados si no existe\n",
    "results_dir = project_root / \"data\" / \"02_processed\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Preparar datos para exportación (convertir a formato JSON-serializable)\n",
    "export_data = {\n",
    "    'analysis_metadata': {\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'sprint': '1.1 - Rigorous EDA',\n",
    "        'dataset': 'IEEE-CIS Fraud Detection',\n",
    "        'total_samples': len(explorer.merged_train),\n",
    "        'total_features': len(explorer.merged_train.columns)\n",
    "    },\n",
    "    'key_findings': {\n",
    "        'fraud_rate': target_results['basic_distribution']['fraud_rate'],\n",
    "        'class_imbalance_ratio': target_results['basic_distribution']['class_imbalance_ratio'],\n",
    "        'features_with_missing': missing_results['merged_train']['features_with_missing'],\n",
    "        'heavily_missing_features': missing_results['merged_train']['heavily_missing_features'],\n",
    "        'multicollinear_pairs': len(correlation_results.get('high_correlations', []))\n",
    "    },\n",
    "    'top_predictive_features': list(correlation_results['target_correlations']['strongest_correlations'].keys())[:20] if 'target_correlations' in correlation_results else [],\n",
    "    'temporal_insights': {\n",
    "        'peak_fraud_hour': target_results.get('temporal_patterns', {}).get('peak_fraud_hour', None),\n",
    "        'peak_fraud_day': target_results.get('temporal_patterns', {}).get('peak_fraud_day', None)\n",
    "    } if 'temporal_patterns' in target_results else {},\n",
    "    'preprocessing_recommendations': [\n",
    "        'Remove features with >95% missing values',\n",
    "        'Implement sophisticated imputation for important features with moderate missingness',\n",
    "        'Apply class balancing techniques (SMOTE, class weights, or undersampling)',\n",
    "        'Create temporal features (hour, day, week patterns)',\n",
    "        'Engineer interaction features from top predictive features',\n",
    "        'Apply feature selection to handle multicollinearity',\n",
    "        'Normalize/standardize numerical features',\n",
    "        'Encode categorical features with target encoding for high-cardinality variables'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Guardar resultados\n",
    "output_file = results_dir / \"eda_findings_sprint_1_1.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(export_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"💾 Resultados exportados a: {output_file}\")\n",
    "print(f\"📊 Tamaño del archivo: {output_file.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Guardar lista de features importantes\n",
    "features_file = results_dir / \"important_features_sprint_1_1.txt\"\n",
    "with open(features_file, 'w') as f:\n",
    "    f.write(\"# Features más importantes identificadas en EDA Sprint 1.1\\n\")\n",
    "    f.write(f\"# Generado: {datetime.now().isoformat()}\\n\\n\")\n",
    "    \n",
    "    if 'target_correlations' in correlation_results:\n",
    "        f.write(\"## Top 20 Features por Correlación con isFraud:\\n\")\n",
    "        for i, (feature, corr) in enumerate(correlation_results['target_correlations']['strongest_correlations'].items()):\n",
    "            if i >= 20:\n",
    "                break\n",
    "            f.write(f\"{i+1:2d}. {feature:<30} {corr:8.4f}\\n\")\n",
    "\n",
    "print(f\"📋 Lista de features importantes guardada en: {features_file}\")\n",
    "print(\"\\n✅ SPRINT 1.1 COMPLETADO EXITOSAMENTE!\")\n",
    "print(\"📈 Datos listos para Sprint 1.2 - Feature Engineering\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
