# AEGIS Fraud Detection System - Project Summary

> **Sistema de DetecciÃ³n de Fraude Transaccional** 
> *Proyecto de Machine Learning con enfoque en imbalanced datasets*

---

## ğŸ“Š Project Overview

**Objetivo:** Desarrollar un sistema de ML robusto para detectar transacciones fraudulentas en datasets altamente desbalanceados.

**Dataset:** IEEE-CIS Fraud Detection (Kaggle)
- **Size:** 590,540 transacciones
- **Features:** 394 variables (transaccionales, de identidad, categÃ³ricas)
- **Target:** `isFraud` (binario)
- **Imbalance Ratio:** ~3.5% fraude (alta dificultad)

**TecnologÃ­as:** Python, LightGBM, XGBoost, CatBoost, MLflow, Optuna, SMOTE

---

## ğŸ¯ Key Achievements

### ğŸ† Performance Highlights
- **Best PR-AUC:** `0.6347` (+5.40% vs baseline)
- **Best F1-Score:** `0.6061` (+12.16% vs baseline)  
- **Model:** LightGBM optimizado con Optuna
- **First model >50% F1-Score:** âœ… Achieved

### ğŸš€ Technical Milestones
- âœ… **Data Engineering Pipeline** completado
- âœ… **SMOTE Strategy Optimization** (+76.3% F1 improvement)
- âœ… **Advanced Models Comparison** (3 gradient boosting)
- âœ… **Bayesian Hyperparameter Optimization** (75 trials)
- âœ… **MLflow Experiment Tracking** (6+ experiments)

---

## ğŸ“‹ Sprint Progress

```mermaid
gantt
    title AEGIS Development Timeline
    dateFormat  YYYY-MM-DD
    section Sprint 2.1
    Data Exploration     :done, s21, 2024-12-30, 1d
    EDA & Preprocessing  :done, 2024-12-30, 1d
    section Sprint 2.2
    SMOTE Strategies     :done, s22, 2024-12-30, 1d
    Pipeline Optimization:done, 2024-12-30, 1d
    section Sprint 2.3
    Advanced Models      :done, s23, 2024-12-31, 1d
    Model Comparison     :done, 2024-12-31, 1d
    section Sprint 2.4
    Hyperparameter Opt   :done, s24, 2025-08-21, 1d
    Bayesian Search      :done, 2025-08-21, 1d
    section Sprint 2.5
    Feature Engineering  :active, s25, 2025-08-22, 1d
```

### ğŸƒ Sprint 2.1: Data Exploration & Preprocessing
**Status:** âœ… Completed  
**Duration:** 1 dÃ­a  

**Deliverables:**
- ğŸ“Š Comprehensive EDA con 394 features
- ğŸ§¹ Data cleaning pipeline (missing values, outliers)
- ğŸ“ˆ Imbalance analysis (3.5% fraud rate)
- ğŸ” Feature correlation analysis

**Key Insights:**
- High missing values in identity features (>80%)
- Strong temporal patterns in transactions
- Card features show fraud indicators
- Geographic clustering of fraudulent activity

**Artifacts:**
- `data/02_processed/fraud_train_v1.0.parquet` (40k samples)
- `docs/eda/fraud_detection_eda_report.html`

---

### ğŸƒ Sprint 2.2: SMOTE Strategy Optimization
**Status:** âœ… Completed  
**Duration:** 1 dÃ­a  

**Objective:** Optimizar estrategias de sampling para datasets desbalanceados

**Tested Strategies:**
| Strategy | Sampling Ratio | F1-Score | PR-AUC | Result |
|----------|---------------|----------|---------|--------|
| **SMOTE Conservative** | 0.5 | **0.5404** | **0.6022** | ğŸ† **Winner** |
| SMOTE Moderate | 0.7 | 0.4892 | 0.5743 | Good |
| SMOTE Aggressive | 1.0 | 0.4156 | 0.5234 | Overfitting |
| BorderlineSMOTE | 0.5 | 0.4823 | 0.5891 | Competitive |
| ADASYN | 0.5 | 0.4567 | 0.5456 | Underfits |

**Key Findings:**
- **SMOTE Conservative** (50% sampling) optimal balance
- Aggressive oversampling causes overfitting
- Cross-validation essential for robust evaluation
- **+76.3% F1-Score improvement** over baseline

**Best Pipeline:**
```
StandardScaler â†’ SMOTE(ratio=0.5, k=3) â†’ LightGBM
```

---

### ğŸƒ Sprint 2.3: Advanced Models & Comparison
**Status:** âœ… Completed  
**Duration:** 1 dÃ­a  

**Objective:** Evaluar modelos avanzados de gradient boosting

**Models Tested:**
| Model | F1-Score | PR-AUC | Precision | Recall | Training Time |
|-------|----------|---------|-----------|--------|---------------|
| **LightGBM** | **0.5404** | **0.6022** | 0.4574 | 0.6667 | 12.3s |
| XGBoost | 0.5156 | 0.5823 | 0.4289 | 0.6508 | 18.7s |
| CatBoost | 0.4967 | 0.5645 | 0.4198 | 0.6032 | 45.2s |
| Random Forest | 0.4234 | 0.4892 | 0.3567 | 0.5238 | 8.9s |
| Logistic Regression | 0.3067 | 0.4621 | 0.2843 | 0.3333 | 2.1s |

**Ensemble Results:**
- **Voting Classifier:** PR-AUC 0.6052 (+0.5% vs LightGBM)
- **Cost:** 2.3x training time
- **Conclusion:** LightGBM sufficient as single model

**Key Insights:**
- LightGBM dominates in speed + performance
- Gradient boosting >> traditional models for fraud detection
- Ensemble gains marginal vs computational cost
- **First model achieving >50% F1-Score consistently**

---

### ğŸƒ Sprint 2.4: Hyperparameter Optimization  
**Status:** âœ… Completed  
**Duration:** 1 dÃ­a  

**Objective:** Optimizar LightGBM usando Bayesian optimization (Optuna)

**Optimization Setup:**
- **Algorithm:** Optuna TPE (Tree-structured Parzen Estimator)
- **Trials:** 75 completed (25 minutes runtime)
- **Objective:** PR-AUC maximization + time penalty
- **Cross-validation:** 3-fold StratifiedKFold
- **Search Space:** 12 hyperparameters

**Results:**
| Metric | Baseline | Optimized | Improvement |
|--------|----------|-----------|-------------|
| **PR-AUC** | 0.6022 Â± 0.0256 | **0.6347 Â± 0.0143** | **+5.40%** |
| **F1-Score** | 0.5404 Â± 0.0289 | **0.6061 Â± 0.0155** | **+12.16%** |
| **Precision** | 0.4574 | 0.5234 | +14.43% |
| **Recall** | 0.6667 | 0.7143 | +7.14% |

**ğŸ¯ Optimal Hyperparameters:**
```json
{
  "n_estimators": 289,
  "learning_rate": 0.201372733219607,
  "num_leaves": 78,
  "max_depth": 13,
  "feature_fraction": 0.8679263850863104,
  "bagging_fraction": 0.9508858341419435,
  "bagging_freq": 1,
  "min_data_in_leaf": 50,
  "lambda_l1": 0.41681282860035934,
  "lambda_l2": 1.7122803598701877,
  "min_split_gain": 0.08740305329369497,
  "min_child_weight": 0.10259402884917516
}
```

**ğŸ“Š Parameter Importance:**
1. `max_depth`: 17.2%
2. `n_estimators`: 12.6%  
3. `bagging_fraction`: 11.4%
4. `feature_fraction`: 11.2%
5. `learning_rate`: 10.8%

**Key Insights:**
- Tree depth most critical for fraud detection
- Moderate regularization prevents overfitting
- High feature/bagging fractions optimal
- Bayesian optimization found global optimum efficiently

---

## ğŸ”¬ Technical Architecture

### ğŸ“Š Data Pipeline
```
Raw Data (590k transactions)
    â†“
Feature Engineering
    â†“
Train/Validation Split (80/20)
    â†“
StandardScaler â†’ SMOTE(0.5) â†’ LightGBM
    â†“
Model Evaluation (CV + Holdout)
```

### ğŸ—ï¸ Model Architecture
**Base Pipeline:**
- **Preprocessing:** StandardScaler (numerical features)
- **Sampling:** SMOTE Conservative (ratio=0.5, k_neighbors=3)
- **Model:** LightGBM Classifier (optimized hyperparameters)
- **Evaluation:** StratifiedKFold CV + holdout test

**Advanced Features:**
- MLflow experiment tracking
- Optuna hyperparameter optimization
- Cross-validation with stratification
- Multi-metric evaluation (PR-AUC, F1, Precision, Recall)

### ğŸ› ï¸ Technology Stack
| Component | Technology | Purpose |
|-----------|------------|---------|
| **Core ML** | LightGBM 4.6.0 | Primary classifier |
| **Optimization** | Optuna 4.1.0 | Bayesian hyperparameter search |
| **Tracking** | MLflow 3.3.1 | Experiment management |
| **Sampling** | SMOTE (imbalanced-learn) | Synthetic minority oversampling |
| **Data** | Pandas, NumPy | Data manipulation |
| **Visualization** | Matplotlib, Seaborn, Plotly | Results visualization |

---

## ğŸ“ˆ Performance Evolution

### ğŸ¯ Model Performance Journey
```
Sprint 2.1: Baseline LR â†’ F1: 0.3067, PR-AUC: 0.4621
Sprint 2.2: SMOTE + LightGBM â†’ F1: 0.5404 (+76.3%), PR-AUC: 0.6022 (+30.4%)
Sprint 2.3: Advanced Models â†’ F1: 0.5404 (maintained), PR-AUC: 0.6022 (confirmed)
Sprint 2.4: Hyperopt LightGBM â†’ F1: 0.6061 (+12.16%), PR-AUC: 0.6347 (+5.40%)
```

### ğŸ“Š Key Metrics Timeline
| Sprint | Model | F1-Score | PR-AUC | Improvement |
|--------|-------|----------|---------|-------------|
| 2.1 | Logistic Regression | 0.3067 | 0.4621 | Baseline |
| 2.2 | LightGBM + SMOTE | 0.5404 | 0.6022 | +76.3% F1 |
| 2.3 | Model Comparison | 0.5404 | 0.6022 | Confirmed |
| 2.4 | **Optimized LightGBM** | **0.6061** | **0.6347** | **+97.6% F1** |

**ğŸ† Total Improvement from Baseline:** 
- **F1-Score: +97.6%** (0.3067 â†’ 0.6061)
- **PR-AUC: +37.4%** (0.4621 â†’ 0.6347)

---

## ğŸ§ª Experiment Tracking

### ğŸ“Š MLflow Experiments
| Experiment | Runs | Best PR-AUC | Best F1 | Purpose |
|------------|------|-------------|---------|---------|
| fraud-detection-smote-sprint-2-2 | 5 | 0.6022 | 0.5404 | SMOTE optimization |
| fraud-detection-advanced-sprint-2-3 | 6 | 0.6052 | 0.5404 | Model comparison |
| fraud-detection-hyperopt-sprint-2-4 | 75+ | 0.6347 | 0.6061 | Hyperparameter opt |

### ğŸ—„ï¸ Artifacts Repository
```
docs/
â”œâ”€â”€ sprints/
â”‚   â”œâ”€â”€ Sprint_2_1_COMPLETED.md
â”‚   â”œâ”€â”€ Sprint_2_2_COMPLETED.md
â”‚   â”œâ”€â”€ Sprint_2_3_COMPLETED.md
â”‚   â””â”€â”€ optimization_results/
â”‚       â”œâ”€â”€ best_lightgbm_model_20250821_165934.pkl
â”‚       â”œâ”€â”€ best_hyperparameters_20250821_165934.json
â”‚       â”œâ”€â”€ optimization_history_20250821_165934.csv
â”‚       â”œâ”€â”€ optimization_report_20250821_165934.md
â”‚       â”œâ”€â”€ optimization_progress.png
â”‚       â”œâ”€â”€ parameter_importance.png
â”‚       â””â”€â”€ parameter_correlations.png
â”œâ”€â”€ eda/
â”‚   â””â”€â”€ fraud_detection_eda_report.html
â””â”€â”€ AEGIS_PROJECT_SUMMARY.md
```

---

## ğŸ¯ Business Impact

### ğŸ’° Fraud Detection Capability
**Current Model Performance:**
- **Precision: 52.34%** â†’ 1 en 2 alertas es fraude real
- **Recall: 71.43%** â†’ Detecta 7 de cada 10 fraudes
- **F1-Score: 60.61%** â†’ Balance Ã³ptimo precision/recall

**Business Metrics (estimados):**
- **False Positive Rate:** ~11% (reducciÃ³n de alertas innecesarias)
- **Detection Rate:** 71.4% de fraudes identificados
- **Cost Savings:** Significativa reducciÃ³n en investigaciÃ³n manual

### ğŸ² Risk Assessment
**Model Strengths:**
- âœ… Robust cross-validation (generalization)
- âœ… Optimized for imbalanced data (SMOTE)
- âœ… Fast inference (<1ms per transaction)
- âœ… Explainable gradient boosting

**Areas for Improvement:**
- ğŸ”„ Feature engineering (Sprint 2.5)
- ğŸ”„ Advanced ensemble methods
- ğŸ”„ Real-time deployment pipeline
- ğŸ”„ Concept drift monitoring

---

## ğŸš€ Next Steps: Sprint 2.5

### ğŸ¯ Planned Objectives
**Focus:** Advanced Feature Engineering
- **Duration:** 2-3 dÃ­as
- **Goal:** +10% additional performance gain

**Feature Engineering Pipeline:**
1. **Temporal Features**
   - Transaction velocity patterns
   - Time-based aggregations
   - Seasonal decomposition

2. **Aggregation Features**
   - User behavior patterns
   - Card usage statistics
   - Merchant risk scores

3. **Interaction Features**
   - Cross-feature combinations
   - Domain-specific ratios
   - Risk indicators

4. **Advanced Techniques**
   - Target encoding for categories
   - Polynomial features
   - PCA dimensionality reduction

**Expected Outcomes:**
- Target PR-AUC: >0.67 (+5-10% improvement)
- Target F1-Score: >0.65 (+7-10% improvement)
- Enhanced feature interpretability

---

## ğŸ“š Knowledge Base

### ğŸ§  Key Learnings
1. **SMOTE Conservative** (50% ratio) optimal for fraud detection
2. **LightGBM** superior to XGBoost/CatBoost for this use case
3. **Tree depth** most important hyperparameter for fraud patterns
4. **Bayesian optimization** highly effective for hyperparameter tuning
5. **Cross-validation** essential for imbalanced dataset evaluation

### âš ï¸ Technical Challenges Solved
- **Imbalanced Data:** SMOTE Conservative strategy
- **Hyperparameter Space:** Optuna TPE sampler
- **Model Selection:** Systematic comparison framework
- **Evaluation Metrics:** PR-AUC focus over accuracy
- **Overfitting:** Proper CV + regularization

### ğŸ”— References & Resources
- **Dataset:** [IEEE-CIS Fraud Detection (Kaggle)](https://www.kaggle.com/c/ieee-fraud-detection)
- **SMOTE Paper:** Chawla et al. (2002) - Synthetic Minority Oversampling
- **LightGBM:** Ke et al. (2017) - Gradient Boosting Framework
- **Optuna:** Akiba et al. (2019) - Hyperparameter Optimization

---

## ğŸ“ Project Status

**Current Phase:** âœ… Sprint 2.4 Completed  
**Next Milestone:** ğŸ¯ Sprint 2.5 Feature Engineering  
**Project Health:** ğŸŸ¢ Excellent Progress  
**Timeline:** ğŸŸ¢ On Track  

**Team:** AEGIS Fraud Detection Team  
**Last Updated:** August 21, 2025  
**Version:** 2.4.0

---

*Este documento es un living summary del proyecto AEGIS. Se actualiza automÃ¡ticamente con cada sprint completado.*

## Tags
`#fraud-detection` `#machine-learning` `#lightgbm` `#optuna` `#smote` `#imbalanced-data` `#gradient-boosting` `#mlflow` `#bayesian-optimization`
