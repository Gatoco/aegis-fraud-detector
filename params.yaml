# Aegis Fraud Detection System - DVC Parameters
# Centralized configuration for data pipeline and model training

# Data Processing Configuration
data_processing:
  # Data validation thresholds
  validation_threshold: 0.95
  missing_value_strategy: "iterative"  # iterative, mean, median, most_frequent
  outlier_detection_method: "isolation_forest"  # isolation_forest, one_class_svm, elliptic_envelope
  outlier_contamination: 0.1
  
  # Data splitting configuration
  train_size: 0.8
  validation_size: 0.1
  test_size: 0.1
  stratify: true
  random_state: 42
  
  # Memory optimization
  chunk_size: 50000
  optimize_dtypes: true
  reduce_memory_usage: true

# Feature Engineering Configuration
feature_engineering:
  # Temporal feature windows (in days)
  temporal_windows: [1, 3, 7, 14, 30, 90]
  
  # Aggregation functions for time-based features
  aggregation_functions: 
    - "mean"
    - "std" 
    - "count"
    - "max"
    - "min"
    - "median"
    - "skew"
    - "kurt"
  
  # Feature interaction settings
  interaction_degree: 2
  polynomial_features: false
  max_interaction_features: 100
  
  # Categorical encoding
  enable_target_encoding: true
  target_encoding_smoothing: 10
  max_categories_for_onehot: 10
  
  # Feature selection
  enable_feature_selection: true
  selection_method: "mutual_info"  # mutual_info, chi2, f_classif
  max_features: 500
  correlation_threshold: 0.95

# Baseline Model Configuration
baseline:
  model_type: "logistic_regression"
  hyperparameters:
    C: 0.01
    class_weight: "balanced"
    max_iter: 1000
    solver: "liblinear"
    random_state: 42
  
  # Cross-validation settings
  cv_folds: 5
  cv_scoring: "roc_auc"

# Ensemble Model Configuration
ensemble:
  # XGBoost configuration
  xgboost:
    n_estimators: 1000
    max_depth: 6
    learning_rate: 0.1
    subsample: 0.8
    colsample_bytree: 0.8
    colsample_bylevel: 0.8
    scale_pos_weight: 20  # Adjusted for class imbalance
    reg_alpha: 0.1
    reg_lambda: 1.0
    random_state: 42
    n_jobs: -1
    tree_method: "hist"
    early_stopping_rounds: 100
    eval_metric: "auc"
  
  # LightGBM configuration
  lightgbm:
    n_estimators: 1000
    max_depth: 6
    learning_rate: 0.1
    subsample: 0.8
    colsample_bytree: 0.8
    min_child_samples: 20
    reg_alpha: 0.1
    reg_lambda: 0.1
    is_unbalance: true
    random_state: 42
    n_jobs: -1
    early_stopping_rounds: 100
    metric: "auc"
    verbosity: -1
  
  # CatBoost configuration  
  catboost:
    iterations: 1000
    depth: 6
    learning_rate: 0.1
    l2_leaf_reg: 3
    auto_class_weights: "Balanced"
    random_seed: 42
    thread_count: -1
    early_stopping_rounds: 100
    eval_metric: "AUC"
    verbose: False
  
  # Stacking meta-learner configuration
  stacking_meta_learner:
    model_type: "logistic_regression"
    hyperparameters:
      C: 0.1
      class_weight: "balanced"
      max_iter: 1000
      random_state: 42
  
  # Ensemble weights (if using weighted averaging)
  ensemble_weights:
    xgboost: 0.4
    lightgbm: 0.4
    catboost: 0.2

# Cross-Validation Configuration
cross_validation:
  n_splits: 5
  shuffle: true
  stratify: true
  random_state: 42
  
  # Time series cross-validation (if applicable)
  time_series_split: false
  max_train_size: null
  test_size: 10000

# Hyperparameter Optimization
hyperparameter_optimization:
  enable: true
  method: "optuna"  # optuna, hyperopt, sklearn_search
  n_trials: 100
  timeout: 3600  # seconds
  
  # Optuna specific settings
  optuna:
    sampler: "TPE"
    pruner: "MedianPruner"
    direction: "maximize"
    metric: "roc_auc"
  
  # Search spaces for optimization
  search_spaces:
    xgboost:
      n_estimators: [500, 2000]
      max_depth: [3, 10]
      learning_rate: [0.01, 0.3]
      subsample: [0.6, 1.0]
      colsample_bytree: [0.6, 1.0]
      reg_alpha: [0.0, 1.0]
      reg_lambda: [0.0, 1.0]

# Model Evaluation Configuration
evaluation:
  # Primary evaluation metric
  target_metric: "roc_auc"
  target_threshold: 0.87  # Minimum acceptable performance
  
  # Threshold optimization
  threshold_optimization_metric: "f1"  # f1, precision, recall, custom
  threshold_range: [0.1, 0.9]
  threshold_steps: 0.01
  
  # Business metrics
  business_cost_ratio: 10  # Cost of FN vs FP
  revenue_per_transaction: 50  # Average transaction value
  fraud_investigation_cost: 25  # Cost to investigate suspicious transaction
  
  # Evaluation metrics to track
  metrics_to_track:
    - "accuracy"
    - "precision" 
    - "recall"
    - "f1_score"
    - "roc_auc"
    - "average_precision"
    - "log_loss"
    - "false_positive_rate"
    - "false_negative_rate"

# Model Interpretation Configuration
interpretation:
  # SHAP analysis settings
  shap:
    enable: true
    sample_size: 1000  # Number of samples for SHAP calculation
    max_display_features: 20
    
  # LIME analysis settings  
  lime:
    enable: true
    sample_size: 100
    num_features: 10
    
  # Permutation importance
  permutation_importance:
    enable: true
    n_repeats: 10
    random_state: 42

# Data Drift Detection
drift_detection:
  enable: true
  reference_window: 30  # days
  detection_window: 7   # days
  drift_threshold: 0.05
  methods:
    - "kolmogorov_smirnov"
    - "population_stability_index"
    - "jensen_shannon_distance"

# Model Monitoring Configuration
monitoring:
  # Performance monitoring
  performance_degradation_threshold: 0.05
  min_samples_for_evaluation: 1000
  
  # Prediction monitoring
  prediction_drift_threshold: 0.1
  confidence_threshold: 0.8
  
  # Data quality monitoring
  missing_value_threshold: 0.1
  outlier_threshold: 0.05

# Deployment Configuration
deployment:
  model_format: "pickle"  # pickle, joblib, mlflow
  compression: true
  model_registry: "mlflow"
  
  # API configuration
  api:
    max_batch_size: 1000
    timeout: 30  # seconds
    enable_caching: true
    cache_ttl: 300  # seconds
  
  # Infrastructure
  infrastructure:
    cpu_limit: "2"
    memory_limit: "4Gi"
    replicas: 3
    auto_scaling: true

# Logging Configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_to_file: true
  log_file: "logs/aegis_fraud_detection.log"
  max_log_size: "10MB"
  backup_count: 5
